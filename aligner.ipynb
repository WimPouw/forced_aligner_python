{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3248058",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3500978",
   "metadata": {},
   "source": [
    "your_project/\n",
    "├── audio_files/\n",
    "│   ├── 001.wav\n",
    "│   ├── 002.wav\n",
    "│   └── ...\n",
    "└── transcript_files/\n",
    "    ├── transcripts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11930723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created/ensured:\n",
      " - Normalized Audio: ./normalized_audio\n",
      " - Trimmed Audio & Texts: ./trimmed_audio\n",
      " - Aligned Output: ./aligned_output\n",
      " - Temporary Files: ./textgrid\n",
      " - Metadata: ./metadata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textgrid # Used for TextGrid manipulation\n",
    "\n",
    "# New dependencies for pure Python audio processing\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import webrtcvad\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set up your input directories here\n",
    "AUDIO_INPUT_DIR = \"./audio\" # Folder containing original audio files (e.g., 001.wav, 002.mp3)\n",
    "TRANSCIPT_INPUT_DIR = \"./transcripts\" # Folder containing transcript files (e.g., 001.csv, 002.txt)\n",
    "\n",
    "# Set up output and temporary directories\n",
    "NORMALIZED_DIR = \"./normalized_audio\" # Will contain normalized audio files (16kHz mono WAV)\n",
    "TRIMMED_DIR = \"./trimmed_audio\" # Will contain trimmed audio (just speech sections) and corresponding text files\n",
    "OUTPUT_DIR = \"./aligned_output\" # For alignment results (TextGrids and CSVs)\n",
    "TEMP_DIR = \"./textgrid\" # For temporary MFA files and ffmpeg outputs\n",
    "METADATA_DIR = \"./metadata\" # For timing metadata generated during audio processing\n",
    "\n",
    "# --- Create directories if they don't exist ---\n",
    "os.makedirs(NORMALIZED_DIR, exist_ok=True)\n",
    "os.makedirs(TRIMMED_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Directories created/ensured:\")\n",
    "print(f\" - Normalized Audio: {NORMALIZED_DIR}\")\n",
    "print(f\" - Trimmed Audio & Texts: {TRIMMED_DIR}\")\n",
    "print(f\" - Aligned Output: {OUTPUT_DIR}\")\n",
    "print(f\" - Temporary Files: {TEMP_DIR}\")\n",
    "print(f\" - Metadata: {METADATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eb5655",
   "metadata": {},
   "source": [
    "This section defines all the helper functions required for the pipeline. You generally don't need to modify these unless you want to customize the audio processing or alignment logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df9f8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio_pure_python(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert audio to 16kHz mono 16-bit WAV format, apply highpass/lowpass filters,\n",
    "    and normalize loudness.\n",
    "    \"\"\"\n",
    "    print(f\"Normalizing (Pure Python): {os.path.basename(input_file)}\")\n",
    "    try:\n",
    "        data, samplerate = sf.read(str(input_file), dtype='float32') # Added str() for robustness\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if data.ndim > 1:\n",
    "            data = np.mean(data, axis=1)\n",
    "\n",
    "        target_samplerate = 16000\n",
    "\n",
    "        # Resample to 16kHz\n",
    "        if samplerate != target_samplerate:\n",
    "            num = int(data.shape[0] * target_samplerate / samplerate)\n",
    "            data = signal.resample(data, num)\n",
    "            samplerate = target_samplerate\n",
    "\n",
    "        # --- LOUDNESS ADJUSTMENT START ---\n",
    "        # Normalize to a peak of 0.95 (before converting to int16)\n",
    "        # This will boost quieter audio without clipping if its original peak is low.\n",
    "        # If the audio is already loud, data.max() will be close to 1.0, and it won't change much.\n",
    "        peak_amplitude = np.max(np.abs(data))\n",
    "        if peak_amplitude > 0: # Avoid division by zero for silent audio\n",
    "            target_peak = 0.95 # Aim for 95% of the maximum possible amplitude\n",
    "            data = data * (target_peak / peak_amplitude)\n",
    "        # --- LOUDNESS ADJUSTMENT END ---\n",
    "\n",
    "        # Ensure data is within valid float range before scaling to int16\n",
    "        data = np.clip(data, -1.0, 1.0) # Clip to avoid overflow after filtering\n",
    "\n",
    "        # Scale to 16-bit PCM range (-32768 to 32767)\n",
    "        data_int16 = (data * 32767).astype(np.int16)\n",
    "\n",
    "        sf.write(str(output_file), data_int16, samplerate) # Added str() for robustness\n",
    "        print(\"Audio normalization succeeded.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error normalizing audio with pure Python: {e}\")\n",
    "        return False\n",
    "\n",
    "def detect_speech_regions_webrtcvad(audio_file, vad_aggressiveness=0):\n",
    "    \"\"\"\n",
    "    Detect speech regions in audio file using WebRTC VAD.\n",
    "    Returns a list of (start_time, end_time) tuples for speech regions.\n",
    "    Assumes 16kHz mono 16-bit WAV input.\n",
    "    \"\"\"\n",
    "    print(f\"Detecting speech (WebRTC VAD): {os.path.basename(audio_file)}\")\n",
    "\n",
    "    vad = webrtcvad.Vad(vad_aggressiveness) # Aggressiveness 0-3 (0 least aggressive, 3 most aggressive)\n",
    "\n",
    "    try:\n",
    "        # Read raw 16-bit PCM data (assuming normalized_audio_pure_python already produced this)\n",
    "        data, samplerate = sf.read(audio_file, dtype='int16')\n",
    "\n",
    "        if samplerate not in [8000, 16000, 32000] or data.ndim > 1:\n",
    "            print(f\"Warning: Audio for VAD is not in WebRTC VAD compatible format (16kHz mono 16-bit). Attempting to convert.\")\n",
    "            # If for some reason the audio wasn't 16kHz mono 16-bit, re-process\n",
    "            # This shouldn't be strictly necessary if normalize_audio_pure_python works correctly.\n",
    "            if data.ndim > 1:\n",
    "                data = np.mean(data, axis=1).astype(np.int16) # Convert to mono\n",
    "            if samplerate != 16000:\n",
    "                num = int(data.shape[0] * 16000 / samplerate)\n",
    "                data = signal.resample(data, num).astype(np.int16)\n",
    "                samplerate = 16000\n",
    "            \n",
    "        raw_audio = data.tobytes() # Convert numpy array to bytes\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio for VAD: {e}\")\n",
    "        return []\n",
    "\n",
    "    frame_duration_ms = 30  # WebRTC VAD operates on 10, 20, or 30 ms frames\n",
    "    bytes_per_frame = int(samplerate * (frame_duration_ms / 1000.0) * 2) # 2 bytes per sample for int16\n",
    "\n",
    "    speech_regions = []\n",
    "    current_speech_start = -1\n",
    "    \n",
    "    # Iterate over frames and check for speech\n",
    "    for i in range(0, len(raw_audio) - bytes_per_frame + 1, bytes_per_frame):\n",
    "        frame = raw_audio[i:i + bytes_per_frame]\n",
    "        is_speech = vad.is_speech(frame, samplerate)\n",
    "        \n",
    "        current_time_seconds = i / (samplerate * 2.0) # Time in seconds (2 bytes/sample)\n",
    "        \n",
    "        if is_speech:\n",
    "            if current_speech_start == -1: # Start of a new speech segment\n",
    "                current_speech_start = current_time_seconds\n",
    "        else: # Not speech\n",
    "            if current_speech_start != -1: # End of a speech segment\n",
    "                speech_regions.append((current_speech_start, current_time_seconds))\n",
    "                current_speech_start = -1 # Reset for next segment\n",
    "\n",
    "    # Add the last speech segment if the audio ends with speech\n",
    "    if current_speech_start != -1:\n",
    "        speech_regions.append((current_speech_start, len(raw_audio) / (samplerate * 2.0)))\n",
    "\n",
    "    # Merge closely spaced speech regions (similar to previous logic)\n",
    "    merged_regions = []\n",
    "    if speech_regions:\n",
    "        current_start, current_end = speech_regions[0]\n",
    "        for i in range(1, len(speech_regions)):\n",
    "            next_start, next_end = speech_regions[i]\n",
    "            # Merge if gap is less than 0.2 seconds\n",
    "            if next_start - current_end < 0.2:\n",
    "                current_end = max(current_end, next_end)\n",
    "            else:\n",
    "                merged_regions.append((current_start, current_end))\n",
    "                current_start, current_end = next_start, next_end\n",
    "        merged_regions.append((current_start, current_end))\n",
    "    speech_regions = merged_regions\n",
    "\n",
    "    # Filter out very short speech segments (e.g., less than 0.5 seconds)\n",
    "    speech_regions = [(s, e) for s, e in speech_regions if (e - s) > 0.5]\n",
    "\n",
    "    if speech_regions:\n",
    "        print(f\"Detected {len(speech_regions)} speech regions:\")\n",
    "        for i, (start, end) in enumerate(speech_regions):\n",
    "            print(f\"  Region {i+1}: {start:.2f}s - {end:.2f}s (duration: {end-start:.2f}s)\")\n",
    "    else:\n",
    "        # Fallback to full duration if no speech detected\n",
    "        try:\n",
    "            full_duration_seconds = len(raw_audio) / (samplerate * 2.0) # 2 bytes/sample for int16\n",
    "            print(f\"No significant speech regions detected. Defaulting to full audio duration: {full_duration_seconds:.2f}s.\")\n",
    "            speech_regions = [(0.0, full_duration_seconds)]\n",
    "        except Exception: # If audio loading itself failed\n",
    "             print(\"Could not determine full audio duration for fallback. Returning empty list.\")\n",
    "             speech_regions = []\n",
    "\n",
    "    return speech_regions\n",
    "\n",
    "def find_main_speech_region(speech_regions, duration_of_full_audio):\n",
    "    \"\"\"\n",
    "    Find the most significant speech region (longest duration).\n",
    "    If no significant speech, defaults to the full audio duration.\n",
    "    \"\"\"\n",
    "    if not speech_regions:\n",
    "        print(\"No speech regions provided. Defaulting to full audio duration.\")\n",
    "        return (0.0, duration_of_full_audio) # Return full duration if no regions\n",
    "\n",
    "    longest_region = max(speech_regions, key=lambda x: x[1] - x[0])\n",
    "    return longest_region\n",
    "\n",
    "def trim_audio_pure_python(input_file, output_file, start_time, end_time):\n",
    "    \"\"\"Trim audio file to extract just the speech section using soundfile.\"\"\"\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    if duration <= 0:\n",
    "        print(f\"Warning: Calculated trim duration is non-positive ({duration:.2f}s). Skipping trim and copying input to output.\")\n",
    "        try:\n",
    "            shutil.copy(input_file, output_file)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying file: {e}\")\n",
    "            return False\n",
    "\n",
    "    try:\n",
    "        data, samplerate = sf.read(input_file, dtype='int16') # Read as int16 as it's normalized WAV\n",
    "        \n",
    "        start_sample = int(start_time * samplerate)\n",
    "        end_sample = int(end_time * samplerate)\n",
    "\n",
    "        trimmed_data = data[start_sample:end_sample]\n",
    "        \n",
    "        sf.write(output_file, trimmed_data, samplerate)\n",
    "        print(f\"Trimmed audio to speech section (Pure Python): {start_time:.2f}s - {end_time:.2f}s (duration: {duration:.2f}s)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error trimming audio with pure Python: {e}\")\n",
    "        print(f\"Attempting to copy full normalized audio as trimmed: {input_file} -> {output_file}\")\n",
    "        try:\n",
    "            shutil.copy(input_file, output_file)\n",
    "            return True # Consider it successful if fallback copy works\n",
    "        except Exception as e_copy:\n",
    "            print(f\"Error copying file after trim failure: {e_copy}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02ebb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initial creation of OUTPUT_DIR_NAME (it's meant to persist)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def perform_full_forced_alignment(audio_file_paths: list[str], \n",
    "                                  transcripts: list[str],\n",
    "                                  mfa_model_name: str = \"dutch_cv\") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Performs the entire forced alignment pipeline on provided audio files and transcript data.\n",
    "    The order of audio_file_paths must correspond to the order of transcripts.\n",
    "\n",
    "    This function coordinates:\n",
    "    1. Per-audio processing (normalization, speech detection, trimming) using Soundfile/SciPy/WebRTC VAD.\n",
    "    2. Creation of individual transcript text files.\n",
    "    3. Execution of the Montreal Forced Aligner (MFA).\n",
    "    4. Post-processing of MFA TextGrids to adjust timings to original audio context.\n",
    "    5. Export of word and phone alignments to CSV files.\n",
    "    6. Cleanup of all temporary and intermediate directories.\n",
    "\n",
    "    Args:\n",
    "        audio_file_paths (list[str]): A list of full paths to the input audio files (e.g., WAV, MP3).\n",
    "        transcripts (list[str]): A list of transcript strings, where the Nth transcript\n",
    "                                 corresponds to the Nth audio file in `audio_file_paths`.\n",
    "        mfa_model_name (str): The name of the MFA acoustic model and dictionary to use\n",
    "                              (e.g., \"english\", \"dutch_cv\"). Defaults to \"dutch_cv\".\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two pandas DataFrames:\n",
    "                                           (word_alignments_df, phone_alignments_df).\n",
    "                                           Returns (None, None) if no successful alignments occurred.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"=== STARTING FULL FORCED ALIGNMENT PROCESS ===\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(audio_file_paths) != len(transcripts):\n",
    "        print(\"Error: The number of audio file paths must match the number of transcripts.\")\n",
    "        return None, None\n",
    "    \n",
    "    if not audio_file_paths:\n",
    "        print(\"No audio files provided. Aborting alignment.\")\n",
    "        return None, None\n",
    "\n",
    "    # Define actual paths for this run's working directories\n",
    "    normalized_dir_path = Path(NORMALIZED_DIR)\n",
    "    trimmed_dir_path = Path(TRIMMED_DIR)\n",
    "    metadata_dir_path = Path(METADATA_DIR)\n",
    "    temp_dir_path = Path(TEMP_DIR) # This is the critical one\n",
    "\n",
    "    # --- Clean up and Recreate temporary directories for THIS run ---\n",
    "    print(\"\\n--- Cleaning up previous run's temporary directories ---\")\n",
    "    #shutil.rmtree(normalized_dir_path, ignore_errors=True)\n",
    "    #shutil.rmtree(trimmed_dir_path, ignore_errors=True)\n",
    "    #shutil.rmtree(metadata_dir_path, ignore_errors=True)\n",
    "    #shutil.rmtree(temp_dir_path, ignore_errors=True) # Clears 'temp' and everything inside it\n",
    "\n",
    "    # Recreate them for the current run\n",
    "    print(\"--- Creating temporary directories for current run ---\")\n",
    "    os.makedirs(normalized_dir_path, exist_ok=True)\n",
    "    os.makedirs(trimmed_dir_path, exist_ok=True)\n",
    "    os.makedirs(metadata_dir_path, exist_ok=True)\n",
    "    os.makedirs(temp_dir_path, exist_ok=True) # Recreates 'temp'\n",
    "\n",
    "    # MFA's raw TextGrid output sub-directory (now guaranteed to have its parent 'temp' existing)\n",
    "    mfa_raw_output_dir = temp_dir_path / \"mfa_raw_output\"\n",
    "    mfa_raw_output_dir.mkdir(exist_ok=True) # This should now succeed\n",
    "    \n",
    "    # --- Initialize containers for results and metadata ---\n",
    "    processed_file_metadata = {} # Stores paths and time_offsets for each file\n",
    "    \n",
    "    # MFA's raw TextGrid output goes here temporarily\n",
    "    mfa_raw_output_dir = Path(TEMP_DIR) / \"mfa_raw_output\"\n",
    "    mfa_raw_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # --- 1. Process Each Audio File & Prepare Transcripts for MFA ---\n",
    "    print(\"\\n--- Processing Audio Files & Preparing Transcripts for MFA ---\")\n",
    "    processed_audio_count = 0\n",
    "\n",
    "    # Combine and sort based on audio file names to ensure consistent processing and association\n",
    "    # We create pairs (audio_path, transcript) and then sort by audio_path base name\n",
    "    # This ensures deterministic processing order regardless of input list order,\n",
    "    # and keeps audio path/transcript pairs together.\n",
    "    combined_inputs = sorted(zip(audio_file_paths, transcripts), key=lambda x: os.path.basename(x[0]))\n",
    "\n",
    "    for i, (original_audio_path, current_transcript_text) in enumerate(combined_inputs):\n",
    "        base_name = os.path.splitext(os.path.basename(original_audio_path))[0]\n",
    "        \n",
    "        current_transcript_text = str(current_transcript_text).strip()\n",
    "        if not current_transcript_text:\n",
    "            print(f\"Warning: Empty transcript provided for audio file {base_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing '{base_name}' from '{original_audio_path}'...\")\n",
    "\n",
    "        # Define specific paths for this file within the global directories\n",
    "        norm_wav_path = os.path.join(NORMALIZED_DIR, f\"{base_name}.wav\")\n",
    "        trimmed_wav_path = os.path.join(TRIMMED_DIR, f\"{base_name}.wav\")\n",
    "        text_file_path = os.path.join(TRIMMED_DIR, f\"{base_name}.txt\") # MFA expects text file in the same directory as audio\n",
    "\n",
    "        # 1.1 Normalize audio (using pure Python/SciPy/Soundfile)\n",
    "        # Call the pure Python normalization function\n",
    "        if not normalize_audio_pure_python(original_audio_path, norm_wav_path):\n",
    "            print(f\"  Failed to normalize {base_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Get normalized audio duration using soundfile\n",
    "        try:\n",
    "            info = sf.info(norm_wav_path)\n",
    "            normalized_audio_duration = info.duration\n",
    "        except Exception as e:\n",
    "            normalized_audio_duration = 0.0\n",
    "            print(f\"  Warning: Could not determine duration for normalized audio '{base_name}' using soundfile: {e}.\")\n",
    "\n",
    "\n",
    "        # 1.2 Detect speech regions (using WebRTC VAD)\n",
    "        # Call the WebRTC VAD based speech detection function\n",
    "        speech_regions = detect_speech_regions_webrtcvad(norm_wav_path)\n",
    "\n",
    "        # 1.3 Find main speech region\n",
    "        main_region = find_main_speech_region(speech_regions, normalized_audio_duration)\n",
    "        \n",
    "        # Ensure main_region is valid for trimming, fallback to full audio if not significant\n",
    "        if main_region[1] - main_region[0] <= 0 and normalized_audio_duration > 0:\n",
    "            print(f\"  No significant speech detected or invalid region for {base_name}. Using full normalized audio for trimming.\")\n",
    "            main_region = (0.0, normalized_audio_duration)\n",
    "        elif normalized_audio_duration == 0:\n",
    "             print(f\"  Cannot determine audio duration for {base_name}. Skipping trimming and alignment for this file.\")\n",
    "             continue # Cannot proceed without a valid duration\n",
    "        \n",
    "        time_offset_seconds = main_region[0] # This is the start of the trimmed segment in the *normalized* audio\n",
    "\n",
    "        # 1.4 Trim audio (using pure Python/Soundfile)\n",
    "        # Call the pure Python trimming function\n",
    "        if not trim_audio_pure_python(norm_wav_path, trimmed_wav_path, main_region[0], main_region[1]):\n",
    "            print(f\"  Failed to trim {base_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 1.5 Write transcript text file (in TRIMMED_DIR next to trimmed audio for MFA)\n",
    "        try:\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(current_transcript_text)\n",
    "            print(f\"  Prepared transcript text for '{base_name}' at '{text_file_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error writing transcript for '{base_name}': {e}. Skipping alignment.\")\n",
    "            continue\n",
    "\n",
    "        # Store metadata for post-processing MFA results\n",
    "        processed_file_metadata[base_name] = {\n",
    "            \"trimmed_audio_path\": trimmed_wav_path,\n",
    "            \"text_file_path\": text_file_path,\n",
    "            \"time_offset_seconds\": time_offset_seconds # Offset from start of normalized audio\n",
    "        }\n",
    "        processed_audio_count += 1\n",
    "\n",
    "    if processed_audio_count == 0:\n",
    "        print(\"\\nNo audio files were successfully prepared for alignment. Aborting pipeline.\")\n",
    "        # Cleanup any partially created directories here\n",
    "        #shutil.rmtree(NORMALIZED_DIR, ignore_errors=True)\n",
    "        #shutil.rmtree(TRIMMED_DIR, ignore_errors=True)\n",
    "        #shutil.rmtree(METADATA_DIR, ignore_errors=True)\n",
    "        #shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "        return None, None\n",
    "\n",
    "    # --- 2. Run MFA Alignment ---\n",
    "    print(\"\\n--- Running Montreal Forced Aligner ---\")\n",
    "    successful_alignments = {}\n",
    "\n",
    "    for base_name, info in processed_file_metadata.items():\n",
    "        trimmed_audio_path = info[\"trimmed_audio_path\"]\n",
    "        text_file_path = info[\"text_file_path\"]\n",
    "        mfa_output_tg = mfa_raw_output_dir / f\"{base_name}.TextGrid\"\n",
    "\n",
    "        if not os.path.exists(trimmed_audio_path) or not os.path.exists(text_file_path):\n",
    "            print(f\"Skipping MFA for '{base_name}': Required trimmed audio or text file missing.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nAligning '{base_name}'...\")\n",
    "        \n",
    "        # *** ADJUSTMENT HERE: Create command as a list of strings ***\n",
    "        mfa_cmd_list = [\n",
    "            'mfa',\n",
    "            'align_one',\n",
    "            '--verbose',\n",
    "            str(trimmed_audio_path),  # Ensure it's a string\n",
    "            str(text_file_path),      # Ensure it's a string\n",
    "            mfa_model_name,\n",
    "            mfa_model_name,\n",
    "            str(mfa_output_tg)        # Ensure it's a string\n",
    "        ]\n",
    "        \n",
    "        print(f\"  Executing MFA command (list format): {' '.join(mfa_cmd_list)}\") # For debugging\n",
    "\n",
    "        try:\n",
    "            # Change shell=True to shell=False, and pass the list directly\n",
    "            result = subprocess.run(mfa_cmd_list, capture_output=True, text=True, check=True)\n",
    "            print(f\"   ✓ Successfully aligned '{base_name}'.\")\n",
    "            successful_alignments[base_name] = {\n",
    "                \"mfa_textgrid\": str(mfa_output_tg),\n",
    "                \"time_offset_seconds\": info[\"time_offset_seconds\"]\n",
    "            }\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"   × Error aligning '{base_name}':\\n{e.stderr}\")\n",
    "            print(f\"   MFA stdout: {e.stdout}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   × Unexpected error during alignment for '{base_name}': {e}\")\n",
    "\n",
    "    if not successful_alignments:\n",
    "        print(\"\\nNo files were successfully aligned by MFA. Aborting pipeline.\")\n",
    "        # Cleanup all directories if MFA failed for everything\n",
    "        #shutil.rmtree(NORMALIZED_DIR, ignore_errors=True)\n",
    "        #shutil.rmtree(TRIMMED_DIR, ignore_errors=True)\n",
    "        #shutil.rmtree(METADATA_DIR, ignore_errors=True)\n",
    "        #shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "        return None, None\n",
    "\n",
    "    # --- 3. Adjust TextGrid Timings and Export Results ---\n",
    "    print(\"\\n--- Adjusting TextGrid Timings and Exporting Results ---\")\n",
    "    all_words_data = []\n",
    "    all_phones_data = []\n",
    "\n",
    "    for base_name, align_info in successful_alignments.items():\n",
    "        mfa_tg_path = align_info[\"mfa_textgrid\"]\n",
    "        time_offset = align_info[\"time_offset_seconds\"] # Offset from start of original audio\n",
    "\n",
    "        if not Path(mfa_tg_path).exists():\n",
    "            print(f\"  MFA TextGrid not found for '{base_name}'. Skipping post-processing for this file.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tg = textgrid.TextGrid.fromFile(mfa_tg_path)\n",
    "\n",
    "            # Create an adjusted TextGrid that reflects timings in the original audio context\n",
    "            # MFA's TextGrid times are relative to the *trimmed* audio.\n",
    "            # We add the time_offset (start of trimmed audio in original) to shift them back.\n",
    "            adjusted_tg = textgrid.TextGrid(name=tg.name, \n",
    "                                            minTime=tg.minTime + time_offset, \n",
    "                                            maxTime=tg.maxTime + time_offset)\n",
    "\n",
    "            for tier in tg.tiers:\n",
    "                adjusted_tier = textgrid.IntervalTier(name=tier.name,\n",
    "                                                      minTime=tier.minTime + time_offset,\n",
    "                                                      maxTime=tier.maxTime + time_offset)\n",
    "                \n",
    "                for interval in tier.intervals:\n",
    "                    original_start = interval.minTime + time_offset\n",
    "                    original_end = interval.maxTime + time_offset\n",
    "                    \n",
    "                    adjusted_interval = textgrid.Interval(\n",
    "                        minTime=original_start,\n",
    "                        maxTime=original_end,\n",
    "                        mark=interval.mark\n",
    "                    )\n",
    "                    adjusted_tier.addInterval(adjusted_interval)\n",
    "\n",
    "                    # Collect data for CSV export\n",
    "                    if tier.name == \"words\" and interval.mark.strip():\n",
    "                        all_words_data.append({\n",
    "                            \"file_id\": base_name,\n",
    "                            \"word\": interval.mark,\n",
    "                            \"start_time_seconds\": original_start,\n",
    "                            \"end_time_seconds\": original_end,\n",
    "                            \"duration_seconds\": interval.maxTime - interval.minTime # Duration is length of word in trimmed segment\n",
    "                        })\n",
    "                    elif tier.name == \"phones\" and interval.mark.strip() and interval.mark != \"sil\": # Exclude 'sil' phones\n",
    "                        all_phones_data.append({\n",
    "                            \"file_id\": base_name,\n",
    "                            \"phone\": interval.mark,\n",
    "                            \"start_time_seconds\": original_start,\n",
    "                            \"end_time_seconds\": original_end,\n",
    "                            \"duration_seconds\": interval.maxTime - interval.minTime # Duration is length of phone in trimmed segment\n",
    "                        })\n",
    "                adjusted_tg.append(adjusted_tier)\n",
    "            \n",
    "            # Save the adjusted TextGrid to the final OUTPUT_DIR\n",
    "            adjusted_tg_output_path = Path(OUTPUT_DIR) / f\"{base_name}_aligned.TextGrid\"\n",
    "            adjusted_tg.write(str(adjusted_tg_output_path))\n",
    "            print(f\"  - Created adjusted TextGrid for '{base_name}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing TextGrid for '{base_name}': {e}. Skipping post-processing for this file.\")\n",
    "\n",
    "    # Create DataFrames\n",
    "    word_alignments_df = pd.DataFrame(all_words_data)\n",
    "    phone_alignments_df = pd.DataFrame(all_phones_data)\n",
    "\n",
    "    # Save DataFrames to CSV in the final OUTPUT_DIR\n",
    "    word_csv_path = Path(OUTPUT_DIR) / \"all_word_alignments.csv\"\n",
    "    phone_csv_path = Path(OUTPUT_DIR) / \"all_phone_alignments.csv\"\n",
    "\n",
    "    if not word_alignments_df.empty:\n",
    "        word_alignments_df.to_csv(word_csv_path, index=False)\n",
    "        print(f\"\\nSaved word alignments to: {word_csv_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo word alignments to save.\")\n",
    "\n",
    "    if not phone_alignments_df.empty:\n",
    "        phone_alignments_df.to_csv(phone_csv_path, index=False)\n",
    "        print(f\"Saved phone alignments to: {phone_csv_path}\")\n",
    "    else:\n",
    "        print(\"No phone alignments to save.\")\n",
    "\n",
    "    # --- 4. Cleanup Temporary Files ---\n",
    "    print(f\"\\n--- Cleaning up temporary directories ---\")\n",
    "    try:\n",
    "        # Using ignore_errors=True for robustness in case some directories are already removed or empty\n",
    "        shutil.rmtree(NORMALIZED_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(TRIMMED_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(METADATA_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(TEMP_DIR, ignore_errors=True) # This also removes mfa_raw_output_dir inside it\n",
    "        print(\"All temporary directories cleaned successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up temporary directories: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"=== FULL FORCED ALIGNMENT PROCESS COMPLETED ===\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return word_alignments_df, phone_alignments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio files to process: ['./audio\\\\audio1_1.wav', './audio\\\\audio1_2.wav']\n",
      "Transcripts for alignment: ['hij had geen zin in het werk', 'zij vond geen sleutels in de tas']\n",
      "\n",
      "==================================================\n",
      "=== STARTING FULL FORCED ALIGNMENT PROCESS ===\n",
      "==================================================\n",
      "\n",
      "--- Cleaning up previous run's temporary directories ---\n",
      "--- Creating temporary directories for current run ---\n",
      "\n",
      "--- Processing Audio Files & Preparing Transcripts for MFA ---\n",
      "\n",
      "Processing 'audio1_1' from './audio\\audio1_1.wav'...\n",
      "Normalizing (Pure Python): audio1_1.wav\n",
      "Audio normalization succeeded.\n",
      "Detecting speech (WebRTC VAD): audio1_1.wav\n",
      "Detected 1 speech regions:\n",
      "  Region 1: 0.54s - 2.37s (duration: 1.83s)\n",
      "Trimmed audio to speech section (Pure Python): 0.54s - 2.37s (duration: 1.83s)\n",
      "  Prepared transcript text for 'audio1_1' at './trimmed_audio\\audio1_1.txt'.\n",
      "\n",
      "Processing 'audio1_2' from './audio\\audio1_2.wav'...\n",
      "Normalizing (Pure Python): audio1_2.wav\n",
      "Audio normalization succeeded.\n",
      "Detecting speech (WebRTC VAD): audio1_2.wav\n",
      "Detected 1 speech regions:\n",
      "  Region 1: 0.93s - 2.70s (duration: 1.77s)\n",
      "Trimmed audio to speech section (Pure Python): 0.93s - 2.70s (duration: 1.77s)\n",
      "  Prepared transcript text for 'audio1_2' at './trimmed_audio\\audio1_2.txt'.\n",
      "\n",
      "--- Running Montreal Forced Aligner ---\n",
      "\n",
      "Aligning 'audio1_1'...\n",
      "  Executing MFA command (list format): mfa align_one --verbose ./trimmed_audio\\audio1_1.wav ./trimmed_audio\\audio1_1.txt dutch_cv dutch_cv textgrid\\mfa_raw_output\\audio1_1.TextGrid\n"
     ]
    }
   ],
   "source": [
    "# Set up your input directories here\n",
    "AUDIO_INPUT_DIR = \"./audio\" # Folder containing original audio files (e.g., 001.wav, 002.mp3)\n",
    "TRANSCIPT_INPUT_DIR = \"./transcripts\" # Folder containing transcript files (e.g., 001.csv, 002.txt)\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "audio_files_list = glob.glob(AUDIO_INPUT_DIR + \"/*.wav\")  # Adjust the glob pattern to match your audio files\n",
    "transcriptfile = glob.glob(TRANSCIPT_INPUT_DIR + \"/*.csv\")  # Adjust the glob pattern to match your transcript files\n",
    "transcript = pd.read_csv(transcriptfile[0])  # Assuming the first CSV contains\n",
    "transcriptlist = transcript['transcript'].tolist()  # Adjust the column name as needed\n",
    "\n",
    "\n",
    "# print both transciptlist and audio_files_list to verify\n",
    "print(\"Audio files to process:\", audio_files_list)\n",
    "print(\"Transcripts for alignment:\", transcriptlist)\n",
    "\n",
    "final_word_alignments, final_phone_alignments = perform_full_forced_alignment(\n",
    "    audio_file_paths=audio_files_list,\n",
    "    transcripts=transcriptlist,\n",
    "    mfa_model_name=\"dutch_cv\" # Change this if you're using a different MFA model\n",
    ")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
